{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "housesRegression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lrodrigocareaga/practice/blob/master/housesRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuevJNCEHRu0",
        "colab_type": "text"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHJMkg8Yi6nd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import keras,os,tarfile,sys,pickle,zipfile\n",
        "\n",
        "#Keras libraries & packages\n",
        "from keras import models,initializers,layers,optimizers\n",
        "from keras import backend as K\n",
        "from keras.utils import to_categorical,plot_model\n",
        "from keras.models import Sequential,Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Input,Dense,Conv2D,advanced_activations,Flatten,concatenate,Dropout,MaxPooling2D,Activation,GlobalAveragePooling2D,BatchNormalization,SeparableConv2D,SpatialDropout2D,LeakyReLU\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications import MobileNet\n",
        "from keras.applications.mobilenet import preprocess_input\n",
        "\n",
        "#Sci-kit libraries\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics import confusion_matrix,roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "#Others\n",
        "import time\n",
        "import matplotlib.image as mpimg\n",
        "import skimage\n",
        "import argparse\n",
        "import locale\n",
        "import sklearn.model_selection\n",
        "import cv2\n",
        "import shapely.wkt\n",
        "import shapely.affinity\n",
        "import seaborn as sns\n",
        "from skimage.io import imshow\n",
        "from shapely.geometry import MultiPolygon, Polygon\n",
        "from collections import defaultdict\n",
        "from shapely.wkt import loads as wkt_loads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_JXoaNRHTar",
        "colab_type": "code",
        "outputId": "715204d0-5762-43ff-dea2-d3887f8ad1a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# Establish the home directory\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "os.chdir('/content/drive/My Drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqGIQrm8ub85",
        "colab_type": "text"
      },
      "source": [
        "## Kaggle Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D20jZGfbSuBs",
        "colab_type": "code",
        "outputId": "f45a353e-8a8a-4cf4-b13e-217f274dc820",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 58
        }
      },
      "source": [
        "#import kaggle.json\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-393e73db-227a-4943-ba73-9f93985fa71f\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-393e73db-227a-4943-ba73-9f93985fa71f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W57X9jMJ82h",
        "colab_type": "code",
        "outputId": "fd18ac94-12b4-40cd-ef1d-5e5281621e9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!ls ~/.kaggle"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uij7FwroJ854",
        "colab_type": "code",
        "outputId": "a5f69992-0708-4f61-9664-09a9a34f0e3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 930
        }
      },
      "source": [
        "!ls -l ~/.kaggle\n",
        "!cat ~/.kaggle/kaggle.json\n",
        "\n",
        "\n",
        "!pip install -q kaggle\n",
        "!pip install -q kaggle-cli\n",
        "\n",
        "#see the possible datasets in kaggle\n",
        "!kaggle datasets list\n",
        "#see the possible competitions in kaggle\n",
        "!kaggle competitions list"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 4\n",
            "-rw------- 1 root root 70 May  5 19:54 kaggle.json\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.3MB 17.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 38.0MB/s \n",
            "\u001b[?25h  Building wheel for kaggle-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "ref                                                          title                                                size  lastUpdated          downloadCount  \n",
            "-----------------------------------------------------------  --------------------------------------------------  -----  -------------------  -------------  \n",
            "ronitf/heart-disease-uci                                     Heart Disease UCI                                     3KB  2018-06-25 11:33:56          40533  \n",
            "bigquery/crypto-ethereum-classic                             Ethereum Classic Blockchain                          70GB  2019-03-20 23:21:25              0  \n",
            "russellyates88/suicide-rates-overview-1985-to-2016           Suicide Rates Overview 1985 to 2016                 396KB  2018-12-01 19:18:25          27959  \n",
            "karangadiya/fifa19                                           FIFA 19 complete player dataset                       2MB  2018-12-21 03:52:59          27347  \n",
            "iarunava/cell-images-for-detecting-malaria                   Malaria Cell Images Dataset                         337MB  2018-12-05 05:40:21           6036  \n",
            "lava18/google-play-store-apps                                Google Play Store Apps                                2MB  2019-02-03 13:55:47          53823  \n",
            "jessicali9530/stanford-dogs-dataset                          Stanford Dogs Dataset                               735MB  2019-02-13 05:45:25           3439  \n",
            "jutrera/stanford-car-dataset-by-classes-folder               Stanford Car Dataset by classes folder                2GB  2018-07-02 07:35:45           3276  \n",
            "jessicali9530/celeba-dataset                                 CelebFaces Attributes (CelebA) Dataset                1GB  2018-06-01 20:08:48           7784  \n",
            "mohansacharya/graduate-admissions                            Graduate Admissions                                   9KB  2018-12-28 10:07:14          22161  \n",
            "safegraph/census-block-group-american-community-survey-data  Census Block Group American Community Survey Data     2GB  2018-12-22 00:29:56            898  \n",
            "cityofLA/los-angeles-parking-citations                       Los Angeles Parking Citations                       261MB  2019-05-04 22:18:08           3832  \n",
            "vjchoudhary7/customer-segmentation-tutorial-in-python        Mall Customer Segmentation Data                       2KB  2018-08-11 07:23:02           9010  \n",
            "noriuk/us-education-datasets-unification-project             U.S. Education Datasets: Unification Project         85MB  2019-03-02 18:41:52           4286  \n",
            "safegraph/visit-patterns-by-census-block-group               Consumer & Visitor Insights For Neighborhoods        66MB  2018-12-19 21:31:50           1750  \n",
            "anokas/kuzushiji                                             Kuzushiji-MNIST                                     318MB  2018-12-17 01:19:31            956  \n",
            "fivethirtyeight/fivethirtyeight-comic-characters-dataset     FiveThirtyEight Comic Characters Dataset            577KB  2019-04-26 15:01:41           2768  \n",
            "rmisra/news-headlines-dataset-for-sarcasm-detection          News Headlines Dataset For Sarcasm Detection          2MB  2018-06-09 22:14:56           3185  \n",
            "pavansanagapati/urban-sound-classification                   Urban Sound Classification                            6GB  2018-06-16 13:44:36           2642  \n",
            "xvivancos/barcelona-data-sets                                Barcelona data sets                                   1MB  2018-12-13 14:16:53           4593  \n",
            "ref                                                deadline             category            reward  teamCount  userHasEntered  \n",
            "-------------------------------------------------  -------------------  ---------------  ---------  ---------  --------------  \n",
            "digit-recognizer                                   2030-01-01 00:00:00  Getting Started  Knowledge       2988           False  \n",
            "titanic                                            2030-01-01 00:00:00  Getting Started  Knowledge      11345           False  \n",
            "house-prices-advanced-regression-techniques        2030-01-01 00:00:00  Getting Started  Knowledge       4542            True  \n",
            "imagenet-object-localization-challenge             2029-12-31 07:00:00  Research         Knowledge         37           False  \n",
            "competitive-data-science-predict-future-sales      2019-12-31 23:59:00  Playground           Kudos       2979           False  \n",
            "two-sigma-financial-news                           2019-07-15 23:59:00  Featured          $100,000       2927           False  \n",
            "aerial-cactus-identification                       2019-07-08 23:59:00  Playground       Knowledge        501           False  \n",
            "jigsaw-unintended-bias-in-toxicity-classification  2019-06-26 23:59:00  Featured           $65,000       1615           False  \n",
            "imaterialist-fashion-2019-FGVC6                    2019-06-10 23:59:00  Research             Kudos         34           False  \n",
            "inaturalist-2019-fgvc6                             2019-06-10 23:59:00  Research             Kudos        115           False  \n",
            "freesound-audio-tagging-2019                       2019-06-10 11:59:00  Research            $5,000        508           False  \n",
            "iwildcam-2019-fgvc6                                2019-06-07 23:59:00  Playground           Kudos        157           False  \n",
            "imet-2019-fgvc6                                    2019-06-04 23:59:00  Research             Kudos        361           False  \n",
            "LANL-Earthquake-Prediction                         2019-06-03 23:59:00  Research           $50,000       3545           False  \n",
            "landmark-recognition-2019                          2019-06-03 23:59:00  Research           $25,000        113           False  \n",
            "landmark-retrieval-2019                            2019-06-03 23:59:00  Research           $25,000         91           False  \n",
            "tmdb-box-office-prediction                         2019-05-30 23:59:00  Playground       Knowledge        960           False  \n",
            "dont-overfit-ii                                    2019-05-07 23:59:00  Playground            Swag       2259           False  \n",
            "ciphertext-challenge-ii                            2019-04-25 23:59:00  Playground            Swag         75           False  \n",
            "data-science-for-good-careervillage                2019-04-23 23:59:00  Analytics          $15,000          0           False  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKlFGMVBJ9Os",
        "colab_type": "code",
        "outputId": "78e2a1fc-8db6-42f5-a842-f222fa8273b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "os.chdir('/content/drive/My Drive/casas/casasBoston')\n",
        "!kaggle competitions list -c house-prices-advanced-regression-techniques\n",
        "!kaggle competitions download -c boston-housing\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: kaggle [-h] [-v] {competitions,c,datasets,d,kernels,k,config} ...\n",
            "kaggle: error: unrecognized arguments: -c house-prices-advanced-regression-techniques\n",
            "Downloading train.csv to /content/drive/My Drive/casas/casasBoston\n",
            "  0% 0.00/23.6k [00:00<?, ?B/s]\n",
            "100% 23.6k/23.6k [00:00<00:00, 3.35MB/s]\n",
            "Downloading test.csv to /content/drive/My Drive/casas/casasBoston\n",
            "  0% 0.00/11.5k [00:00<?, ?B/s]\n",
            "100% 11.5k/11.5k [00:00<00:00, 1.64MB/s]\n",
            "Downloading submission_example.csv to /content/drive/My Drive/casas/casasBoston\n",
            "  0% 0.00/3.52k [00:00<?, ?B/s]\n",
            "100% 3.52k/3.52k [00:00<00:00, 504kB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG87fmeGyT19",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/drive/My Drive/')\n",
        "for file in os.listdir():\n",
        "  if file.endswith(\"csv.zip\"):\n",
        "    zip_ref = zipfile.ZipFile(file, 'r')\n",
        "    zip_ref.extractall()\n",
        "    zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RE00c4pWY-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('deepsat-sat6.zip', 'r')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lYCtiUslYz1",
        "colab_type": "text"
      },
      "source": [
        "## Github Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFgTvnhcmG1C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "248e8bcc-c7bb-428e-e92a-9f3619f8ed6c"
      },
      "source": [
        "!git clone https://github.com/emanhamed/Houses-dataset"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Houses-dataset'...\n",
            "remote: Enumerating objects: 2161, done.\u001b[K\n",
            "remote: Total 2161 (delta 0), reused 0 (delta 0), pack-reused 2161\n",
            "Receiving objects: 100% (2161/2161), 176.26 MiB | 20.60 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n",
            "Checking out files: 100% (2144/2144), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e32AktirJwbc",
        "colab_type": "text"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrY6jnihSbyX",
        "colab_type": "text"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLxz-JTnla6o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cols = [\"bedrooms\", \"bathrooms\", \"area\", \"zipcode\", \"price\"]\n",
        "df = pd.read_csv('/content/drive/My Drive/casas/casasBoston/Houses-dataset/Houses Dataset/HousesInfo.txt', sep=\" \", header=None, names=cols)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD-WmbvHlnce",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c4afeafe-ddf6-424c-e782-e915c5548e7a"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>area</th>\n",
              "      <th>zipcode</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4053</td>\n",
              "      <td>85255</td>\n",
              "      <td>869500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3343</td>\n",
              "      <td>36372</td>\n",
              "      <td>865200.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3923</td>\n",
              "      <td>85266</td>\n",
              "      <td>889000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4022</td>\n",
              "      <td>85262</td>\n",
              "      <td>910000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4116</td>\n",
              "      <td>85266</td>\n",
              "      <td>971226.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   bedrooms  bathrooms  area  zipcode     price\n",
              "0         4        4.0  4053    85255  869500.0\n",
              "1         4        3.0  3343    36372  865200.0\n",
              "2         3        4.0  3923    85266  889000.0\n",
              "3         5        5.0  4022    85262  910000.0\n",
              "4         3        4.0  4116    85266  971226.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ft3Tdy2m9q7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zipcodes = df[\"zipcode\"].value_counts().keys().tolist()\n",
        "counts = df[\"zipcode\"].value_counts().tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo6lN-iXnfi4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for (zipcode, count) in zip(zipcodes, counts):\n",
        "\t\tif count < 25:\n",
        "\t\t\tidxs = df[df[\"zipcode\"] == zipcode].index\n",
        "\t\t\tdf.drop(idxs, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-WH6g4fp_Mf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(train, test) = train_test_split(df, test_size=0.25, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeUa-yM6pXPF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "outputId": "94a8dacf-6a72-430f-b5ab-a5525f3c2065"
      },
      "source": [
        "continuous = [\"bedrooms\", \"bathrooms\", \"area\"]\n",
        " \n",
        "cs = MinMaxScaler()\n",
        "trainContinuous = cs.fit_transform(train[continuous])\n",
        "testContinuous = cs.transform(test[continuous])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
            "  return self.partial_fit(X, y)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4hvy_pOpXMr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zipBinarizer = LabelBinarizer().fit(df[\"zipcode\"])\n",
        "trainCategorical = zipBinarizer.transform(train[\"zipcode\"])\n",
        "testCategorical = zipBinarizer.transform(test[\"zipcode\"])\n",
        "\n",
        "trainX = np.hstack([trainCategorical, trainContinuous])\n",
        "testX = np.hstack([testCategorical, testContinuous])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtPpBXtzrLBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxPrice = train[\"price\"].max()\n",
        "trainY = train[\"price\"] / maxPrice\n",
        "testY = test[\"price\"] / maxPrice"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVMKqPduSi3j",
        "colab_type": "text"
      },
      "source": [
        "## Dense Neural Network for Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfiYVFM-pXKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1 = Sequential()\n",
        "model1.add(Dense(8, input_dim=trainX.shape[1], activation=\"relu\"))\n",
        "model1.add(Dense(4, activation=\"relu\"))\n",
        "model1.add(Dense(1, activation=\"linear\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtIhhKxkpXHa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6851
        },
        "outputId": "0b97a0de-5aaf-4ece-e2a0-0458900f004b"
      },
      "source": [
        "opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
        "model1.compile(optimizer='adam', loss=\"mean_absolute_percentage_error\")\n",
        "\n",
        "\n",
        "start = time.clock() \n",
        "\n",
        "history = model1.fit(trainX, trainY, \n",
        "                    validation_data=(testX, testY),\n",
        "                    epochs=200, \n",
        "                    batch_size=16)\n",
        "end = time.clock()\n",
        "timeDense = end-start\n",
        "\n",
        "print(\"Total Time: {} \".format(timeDense)) "
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 271 samples, validate on 91 samples\n",
            "Epoch 1/200\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 410.2364 - val_loss: 281.4753\n",
            "Epoch 2/200\n",
            "271/271 [==============================] - 0s 266us/step - loss: 215.8306 - val_loss: 138.2611\n",
            "Epoch 3/200\n",
            "271/271 [==============================] - 0s 236us/step - loss: 111.9359 - val_loss: 101.3591\n",
            "Epoch 4/200\n",
            "271/271 [==============================] - 0s 238us/step - loss: 82.8506 - val_loss: 79.2888\n",
            "Epoch 5/200\n",
            "271/271 [==============================] - 0s 259us/step - loss: 63.0408 - val_loss: 56.0147\n",
            "Epoch 6/200\n",
            "271/271 [==============================] - 0s 293us/step - loss: 48.4255 - val_loss: 48.3165\n",
            "Epoch 7/200\n",
            "271/271 [==============================] - 0s 280us/step - loss: 44.1800 - val_loss: 44.7328\n",
            "Epoch 8/200\n",
            "271/271 [==============================] - 0s 241us/step - loss: 40.3557 - val_loss: 41.8082\n",
            "Epoch 9/200\n",
            "271/271 [==============================] - 0s 264us/step - loss: 38.3300 - val_loss: 39.9685\n",
            "Epoch 10/200\n",
            "271/271 [==============================] - 0s 260us/step - loss: 37.1962 - val_loss: 39.3214\n",
            "Epoch 11/200\n",
            "271/271 [==============================] - 0s 237us/step - loss: 35.8826 - val_loss: 39.1636\n",
            "Epoch 12/200\n",
            "271/271 [==============================] - 0s 233us/step - loss: 35.3921 - val_loss: 37.5547\n",
            "Epoch 13/200\n",
            "271/271 [==============================] - 0s 233us/step - loss: 34.4853 - val_loss: 36.6907\n",
            "Epoch 14/200\n",
            "271/271 [==============================] - 0s 230us/step - loss: 33.2861 - val_loss: 35.8331\n",
            "Epoch 15/200\n",
            "271/271 [==============================] - 0s 231us/step - loss: 32.9825 - val_loss: 35.4367\n",
            "Epoch 16/200\n",
            "271/271 [==============================] - 0s 228us/step - loss: 32.7583 - val_loss: 35.9367\n",
            "Epoch 17/200\n",
            "271/271 [==============================] - 0s 229us/step - loss: 33.7592 - val_loss: 34.5400\n",
            "Epoch 18/200\n",
            "271/271 [==============================] - 0s 229us/step - loss: 32.7515 - val_loss: 34.9951\n",
            "Epoch 19/200\n",
            "271/271 [==============================] - 0s 231us/step - loss: 31.9041 - val_loss: 35.1126\n",
            "Epoch 20/200\n",
            "271/271 [==============================] - 0s 235us/step - loss: 32.0032 - val_loss: 35.2574\n",
            "Epoch 21/200\n",
            "271/271 [==============================] - 0s 229us/step - loss: 31.6827 - val_loss: 33.2867\n",
            "Epoch 22/200\n",
            "271/271 [==============================] - 0s 287us/step - loss: 31.7186 - val_loss: 33.5996\n",
            "Epoch 23/200\n",
            "271/271 [==============================] - 0s 231us/step - loss: 31.1611 - val_loss: 32.9129\n",
            "Epoch 24/200\n",
            "271/271 [==============================] - 0s 232us/step - loss: 31.1641 - val_loss: 33.5303\n",
            "Epoch 25/200\n",
            "271/271 [==============================] - 0s 237us/step - loss: 30.6301 - val_loss: 32.9393\n",
            "Epoch 26/200\n",
            "271/271 [==============================] - 0s 227us/step - loss: 30.9725 - val_loss: 32.5293\n",
            "Epoch 27/200\n",
            "271/271 [==============================] - 0s 231us/step - loss: 30.8333 - val_loss: 31.9129\n",
            "Epoch 28/200\n",
            "271/271 [==============================] - 0s 234us/step - loss: 31.3627 - val_loss: 31.7401\n",
            "Epoch 29/200\n",
            "271/271 [==============================] - 0s 229us/step - loss: 32.1398 - val_loss: 31.7906\n",
            "Epoch 30/200\n",
            "271/271 [==============================] - 0s 228us/step - loss: 31.0596 - val_loss: 31.2789\n",
            "Epoch 31/200\n",
            "271/271 [==============================] - 0s 224us/step - loss: 30.0223 - val_loss: 31.0876\n",
            "Epoch 32/200\n",
            "271/271 [==============================] - 0s 231us/step - loss: 29.8489 - val_loss: 30.8339\n",
            "Epoch 33/200\n",
            "271/271 [==============================] - 0s 232us/step - loss: 29.7169 - val_loss: 30.5580\n",
            "Epoch 34/200\n",
            "271/271 [==============================] - 0s 222us/step - loss: 29.3146 - val_loss: 30.6186\n",
            "Epoch 35/200\n",
            "271/271 [==============================] - 0s 224us/step - loss: 29.5341 - val_loss: 30.3925\n",
            "Epoch 36/200\n",
            "271/271 [==============================] - 0s 228us/step - loss: 29.2142 - val_loss: 30.2687\n",
            "Epoch 37/200\n",
            "271/271 [==============================] - 0s 236us/step - loss: 29.7296 - val_loss: 30.6573\n",
            "Epoch 38/200\n",
            "271/271 [==============================] - 0s 271us/step - loss: 29.3304 - val_loss: 30.5104\n",
            "Epoch 39/200\n",
            "271/271 [==============================] - 0s 227us/step - loss: 29.7959 - val_loss: 30.0780\n",
            "Epoch 40/200\n",
            "271/271 [==============================] - 0s 231us/step - loss: 28.8374 - val_loss: 30.2781\n",
            "Epoch 41/200\n",
            "271/271 [==============================] - 0s 220us/step - loss: 28.8238 - val_loss: 29.8338\n",
            "Epoch 42/200\n",
            "271/271 [==============================] - 0s 233us/step - loss: 29.1819 - val_loss: 29.9800\n",
            "Epoch 43/200\n",
            "271/271 [==============================] - 0s 239us/step - loss: 28.9891 - val_loss: 30.1931\n",
            "Epoch 44/200\n",
            "271/271 [==============================] - 0s 234us/step - loss: 28.6513 - val_loss: 29.8632\n",
            "Epoch 45/200\n",
            "271/271 [==============================] - 0s 241us/step - loss: 29.1350 - val_loss: 29.9301\n",
            "Epoch 46/200\n",
            "271/271 [==============================] - 0s 234us/step - loss: 29.8066 - val_loss: 29.8065\n",
            "Epoch 47/200\n",
            "271/271 [==============================] - 0s 261us/step - loss: 28.8151 - val_loss: 30.5805\n",
            "Epoch 48/200\n",
            "271/271 [==============================] - 0s 227us/step - loss: 28.8647 - val_loss: 29.5533\n",
            "Epoch 49/200\n",
            "271/271 [==============================] - 0s 235us/step - loss: 28.6640 - val_loss: 30.0825\n",
            "Epoch 50/200\n",
            "271/271 [==============================] - 0s 240us/step - loss: 28.7718 - val_loss: 29.5603\n",
            "Epoch 51/200\n",
            "271/271 [==============================] - 0s 246us/step - loss: 29.1579 - val_loss: 29.5928\n",
            "Epoch 52/200\n",
            "271/271 [==============================] - 0s 248us/step - loss: 29.1547 - val_loss: 29.4630\n",
            "Epoch 53/200\n",
            "271/271 [==============================] - 0s 297us/step - loss: 28.6113 - val_loss: 29.3906\n",
            "Epoch 54/200\n",
            "271/271 [==============================] - 0s 229us/step - loss: 28.9583 - val_loss: 29.1809\n",
            "Epoch 55/200\n",
            "271/271 [==============================] - 0s 232us/step - loss: 29.2639 - val_loss: 29.6864\n",
            "Epoch 56/200\n",
            "271/271 [==============================] - 0s 246us/step - loss: 28.9418 - val_loss: 29.6994\n",
            "Epoch 57/200\n",
            "271/271 [==============================] - 0s 237us/step - loss: 28.4510 - val_loss: 28.9769\n",
            "Epoch 58/200\n",
            "271/271 [==============================] - 0s 231us/step - loss: 28.8996 - val_loss: 29.2116\n",
            "Epoch 59/200\n",
            "271/271 [==============================] - 0s 227us/step - loss: 28.8780 - val_loss: 28.8435\n",
            "Epoch 60/200\n",
            "271/271 [==============================] - 0s 231us/step - loss: 28.3709 - val_loss: 29.3541\n",
            "Epoch 61/200\n",
            "271/271 [==============================] - 0s 217us/step - loss: 29.1987 - val_loss: 29.1891\n",
            "Epoch 62/200\n",
            "271/271 [==============================] - 0s 234us/step - loss: 28.9351 - val_loss: 29.1293\n",
            "Epoch 63/200\n",
            "271/271 [==============================] - 0s 236us/step - loss: 29.2423 - val_loss: 29.1821\n",
            "Epoch 64/200\n",
            "271/271 [==============================] - 0s 226us/step - loss: 28.0704 - val_loss: 29.8281\n",
            "Epoch 65/200\n",
            "271/271 [==============================] - 0s 229us/step - loss: 28.8060 - val_loss: 29.9628\n",
            "Epoch 66/200\n",
            "271/271 [==============================] - 0s 227us/step - loss: 29.4479 - val_loss: 30.4313\n",
            "Epoch 67/200\n",
            "271/271 [==============================] - 0s 229us/step - loss: 30.0173 - val_loss: 32.0817\n",
            "Epoch 68/200\n",
            "271/271 [==============================] - 0s 231us/step - loss: 30.2692 - val_loss: 29.4067\n",
            "Epoch 69/200\n",
            "271/271 [==============================] - 0s 280us/step - loss: 28.9150 - val_loss: 29.1509\n",
            "Epoch 70/200\n",
            "271/271 [==============================] - 0s 238us/step - loss: 27.8912 - val_loss: 29.8349\n",
            "Epoch 71/200\n",
            "271/271 [==============================] - 0s 234us/step - loss: 28.5621 - val_loss: 29.9176\n",
            "Epoch 72/200\n",
            "271/271 [==============================] - 0s 207us/step - loss: 30.5538 - val_loss: 29.1741\n",
            "Epoch 73/200\n",
            "271/271 [==============================] - 0s 196us/step - loss: 28.2463 - val_loss: 29.3462\n",
            "Epoch 74/200\n",
            "271/271 [==============================] - 0s 204us/step - loss: 28.6334 - val_loss: 28.9027\n",
            "Epoch 75/200\n",
            "271/271 [==============================] - 0s 195us/step - loss: 28.5532 - val_loss: 28.7525\n",
            "Epoch 76/200\n",
            "271/271 [==============================] - 0s 199us/step - loss: 28.1611 - val_loss: 29.1449\n",
            "Epoch 77/200\n",
            "271/271 [==============================] - 0s 204us/step - loss: 27.9587 - val_loss: 29.0072\n",
            "Epoch 78/200\n",
            "271/271 [==============================] - 0s 201us/step - loss: 28.1298 - val_loss: 28.5961\n",
            "Epoch 79/200\n",
            "271/271 [==============================] - 0s 205us/step - loss: 29.1160 - val_loss: 29.2321\n",
            "Epoch 80/200\n",
            "271/271 [==============================] - 0s 200us/step - loss: 28.2918 - val_loss: 29.4288\n",
            "Epoch 81/200\n",
            "271/271 [==============================] - 0s 199us/step - loss: 28.3995 - val_loss: 29.1913\n",
            "Epoch 82/200\n",
            "271/271 [==============================] - 0s 202us/step - loss: 28.8635 - val_loss: 28.9724\n",
            "Epoch 83/200\n",
            "271/271 [==============================] - 0s 196us/step - loss: 28.0802 - val_loss: 29.0599\n",
            "Epoch 84/200\n",
            "271/271 [==============================] - 0s 198us/step - loss: 28.5027 - val_loss: 29.4585\n",
            "Epoch 85/200\n",
            "271/271 [==============================] - 0s 200us/step - loss: 27.9959 - val_loss: 29.0082\n",
            "Epoch 86/200\n",
            "271/271 [==============================] - 0s 202us/step - loss: 27.8938 - val_loss: 28.7109\n",
            "Epoch 87/200\n",
            "271/271 [==============================] - 0s 231us/step - loss: 28.2858 - val_loss: 28.7214\n",
            "Epoch 88/200\n",
            "271/271 [==============================] - 0s 206us/step - loss: 27.9145 - val_loss: 29.4096\n",
            "Epoch 89/200\n",
            "271/271 [==============================] - 0s 196us/step - loss: 29.3463 - val_loss: 29.1361\n",
            "Epoch 90/200\n",
            "271/271 [==============================] - 0s 201us/step - loss: 27.8831 - val_loss: 28.9768\n",
            "Epoch 91/200\n",
            "271/271 [==============================] - 0s 197us/step - loss: 29.1905 - val_loss: 29.2222\n",
            "Epoch 92/200\n",
            "271/271 [==============================] - 0s 203us/step - loss: 28.7312 - val_loss: 28.7917\n",
            "Epoch 93/200\n",
            "271/271 [==============================] - 0s 200us/step - loss: 27.8191 - val_loss: 28.9450\n",
            "Epoch 94/200\n",
            "271/271 [==============================] - 0s 194us/step - loss: 28.3480 - val_loss: 29.4252\n",
            "Epoch 95/200\n",
            "271/271 [==============================] - 0s 199us/step - loss: 29.5700 - val_loss: 28.6940\n",
            "Epoch 96/200\n",
            "271/271 [==============================] - 0s 202us/step - loss: 28.7939 - val_loss: 29.6276\n",
            "Epoch 97/200\n",
            "271/271 [==============================] - 0s 196us/step - loss: 27.6685 - val_loss: 28.6559\n",
            "Epoch 98/200\n",
            "271/271 [==============================] - 0s 203us/step - loss: 28.2892 - val_loss: 28.4794\n",
            "Epoch 99/200\n",
            "271/271 [==============================] - 0s 197us/step - loss: 28.2655 - val_loss: 28.6789\n",
            "Epoch 100/200\n",
            "271/271 [==============================] - 0s 206us/step - loss: 27.9214 - val_loss: 28.4195\n",
            "Epoch 101/200\n",
            "271/271 [==============================] - 0s 195us/step - loss: 28.3383 - val_loss: 28.2719\n",
            "Epoch 102/200\n",
            "271/271 [==============================] - 0s 197us/step - loss: 28.5863 - val_loss: 30.6906\n",
            "Epoch 103/200\n",
            "271/271 [==============================] - 0s 198us/step - loss: 27.6354 - val_loss: 28.3941\n",
            "Epoch 104/200\n",
            "271/271 [==============================] - 0s 211us/step - loss: 28.0108 - val_loss: 28.0973\n",
            "Epoch 105/200\n",
            "271/271 [==============================] - 0s 236us/step - loss: 28.3052 - val_loss: 28.2485\n",
            "Epoch 106/200\n",
            "271/271 [==============================] - 0s 198us/step - loss: 28.8803 - val_loss: 27.9944\n",
            "Epoch 107/200\n",
            "271/271 [==============================] - 0s 194us/step - loss: 27.8884 - val_loss: 29.0099\n",
            "Epoch 108/200\n",
            "271/271 [==============================] - 0s 201us/step - loss: 28.4243 - val_loss: 28.0789\n",
            "Epoch 109/200\n",
            "271/271 [==============================] - 0s 195us/step - loss: 27.7650 - val_loss: 28.0310\n",
            "Epoch 110/200\n",
            "271/271 [==============================] - 0s 203us/step - loss: 27.7908 - val_loss: 27.8649\n",
            "Epoch 111/200\n",
            "271/271 [==============================] - 0s 193us/step - loss: 27.9793 - val_loss: 28.0986\n",
            "Epoch 112/200\n",
            "271/271 [==============================] - 0s 200us/step - loss: 28.2076 - val_loss: 27.9616\n",
            "Epoch 113/200\n",
            "271/271 [==============================] - 0s 199us/step - loss: 27.9327 - val_loss: 28.6397\n",
            "Epoch 114/200\n",
            "271/271 [==============================] - 0s 197us/step - loss: 27.5174 - val_loss: 27.6084\n",
            "Epoch 115/200\n",
            "271/271 [==============================] - 0s 198us/step - loss: 27.7573 - val_loss: 27.5901\n",
            "Epoch 116/200\n",
            "271/271 [==============================] - 0s 199us/step - loss: 27.3980 - val_loss: 27.6385\n",
            "Epoch 117/200\n",
            "271/271 [==============================] - 0s 197us/step - loss: 27.8297 - val_loss: 27.6521\n",
            "Epoch 118/200\n",
            "271/271 [==============================] - 0s 198us/step - loss: 28.2472 - val_loss: 27.6396\n",
            "Epoch 119/200\n",
            "271/271 [==============================] - 0s 202us/step - loss: 28.0906 - val_loss: 27.8599\n",
            "Epoch 120/200\n",
            "271/271 [==============================] - 0s 198us/step - loss: 27.3630 - val_loss: 27.5935\n",
            "Epoch 121/200\n",
            "271/271 [==============================] - 0s 196us/step - loss: 27.6724 - val_loss: 27.1443\n",
            "Epoch 122/200\n",
            "271/271 [==============================] - 0s 193us/step - loss: 27.2518 - val_loss: 27.4451\n",
            "Epoch 123/200\n",
            "271/271 [==============================] - 0s 199us/step - loss: 27.3673 - val_loss: 27.4930\n",
            "Epoch 124/200\n",
            "271/271 [==============================] - 0s 257us/step - loss: 28.0515 - val_loss: 28.0539\n",
            "Epoch 125/200\n",
            "271/271 [==============================] - 0s 201us/step - loss: 28.7968 - val_loss: 28.9148\n",
            "Epoch 126/200\n",
            "271/271 [==============================] - 0s 203us/step - loss: 29.6903 - val_loss: 27.7156\n",
            "Epoch 127/200\n",
            "271/271 [==============================] - 0s 197us/step - loss: 27.8504 - val_loss: 26.9717\n",
            "Epoch 128/200\n",
            "271/271 [==============================] - 0s 207us/step - loss: 27.1493 - val_loss: 27.3109\n",
            "Epoch 129/200\n",
            "271/271 [==============================] - 0s 197us/step - loss: 27.4100 - val_loss: 28.9172\n",
            "Epoch 130/200\n",
            "271/271 [==============================] - 0s 206us/step - loss: 27.9321 - val_loss: 27.0408\n",
            "Epoch 131/200\n",
            "271/271 [==============================] - 0s 194us/step - loss: 26.6959 - val_loss: 27.0569\n",
            "Epoch 132/200\n",
            "271/271 [==============================] - 0s 198us/step - loss: 27.3037 - val_loss: 27.0977\n",
            "Epoch 133/200\n",
            "271/271 [==============================] - 0s 200us/step - loss: 26.6508 - val_loss: 27.9699\n",
            "Epoch 134/200\n",
            "271/271 [==============================] - 0s 199us/step - loss: 26.5787 - val_loss: 26.9435\n",
            "Epoch 135/200\n",
            "271/271 [==============================] - 0s 195us/step - loss: 26.4248 - val_loss: 27.3104\n",
            "Epoch 136/200\n",
            "271/271 [==============================] - 0s 197us/step - loss: 26.7934 - val_loss: 26.9829\n",
            "Epoch 137/200\n",
            "271/271 [==============================] - 0s 207us/step - loss: 26.6256 - val_loss: 27.2511\n",
            "Epoch 138/200\n",
            "271/271 [==============================] - 0s 197us/step - loss: 26.6307 - val_loss: 26.9355\n",
            "Epoch 139/200\n",
            "271/271 [==============================] - 0s 197us/step - loss: 26.6813 - val_loss: 27.3020\n",
            "Epoch 140/200\n",
            "271/271 [==============================] - 0s 200us/step - loss: 26.5760 - val_loss: 26.8608\n",
            "Epoch 141/200\n",
            "271/271 [==============================] - 0s 202us/step - loss: 26.3559 - val_loss: 26.8901\n",
            "Epoch 142/200\n",
            "271/271 [==============================] - 0s 227us/step - loss: 26.4201 - val_loss: 26.6166\n",
            "Epoch 143/200\n",
            "271/271 [==============================] - 0s 199us/step - loss: 26.2424 - val_loss: 26.5747\n",
            "Epoch 144/200\n",
            "271/271 [==============================] - 0s 196us/step - loss: 26.3594 - val_loss: 27.6850\n",
            "Epoch 145/200\n",
            "271/271 [==============================] - 0s 199us/step - loss: 26.5063 - val_loss: 26.3938\n",
            "Epoch 146/200\n",
            "271/271 [==============================] - 0s 205us/step - loss: 26.2319 - val_loss: 26.3941\n",
            "Epoch 147/200\n",
            "271/271 [==============================] - 0s 206us/step - loss: 26.3038 - val_loss: 26.9558\n",
            "Epoch 148/200\n",
            "271/271 [==============================] - 0s 205us/step - loss: 25.7904 - val_loss: 26.6723\n",
            "Epoch 149/200\n",
            "271/271 [==============================] - 0s 198us/step - loss: 25.5632 - val_loss: 26.2975\n",
            "Epoch 150/200\n",
            "271/271 [==============================] - 0s 194us/step - loss: 25.4345 - val_loss: 26.5370\n",
            "Epoch 151/200\n",
            "271/271 [==============================] - 0s 198us/step - loss: 25.4750 - val_loss: 25.8595\n",
            "Epoch 152/200\n",
            "271/271 [==============================] - 0s 205us/step - loss: 25.3161 - val_loss: 25.5214\n",
            "Epoch 153/200\n",
            "271/271 [==============================] - 0s 213us/step - loss: 25.0650 - val_loss: 25.8452\n",
            "Epoch 154/200\n",
            "271/271 [==============================] - 0s 210us/step - loss: 25.4358 - val_loss: 25.8780\n",
            "Epoch 155/200\n",
            "271/271 [==============================] - 0s 218us/step - loss: 25.2189 - val_loss: 26.0937\n",
            "Epoch 156/200\n",
            "271/271 [==============================] - 0s 211us/step - loss: 25.4932 - val_loss: 25.7495\n",
            "Epoch 157/200\n",
            "271/271 [==============================] - 0s 202us/step - loss: 25.0889 - val_loss: 26.5472\n",
            "Epoch 158/200\n",
            "271/271 [==============================] - 0s 201us/step - loss: 25.3321 - val_loss: 25.5436\n",
            "Epoch 159/200\n",
            "271/271 [==============================] - 0s 200us/step - loss: 25.2232 - val_loss: 25.6856\n",
            "Epoch 160/200\n",
            "271/271 [==============================] - 0s 255us/step - loss: 24.8590 - val_loss: 25.6254\n",
            "Epoch 161/200\n",
            "271/271 [==============================] - 0s 196us/step - loss: 25.2640 - val_loss: 25.3951\n",
            "Epoch 162/200\n",
            "271/271 [==============================] - 0s 195us/step - loss: 24.6577 - val_loss: 26.4822\n",
            "Epoch 163/200\n",
            "271/271 [==============================] - 0s 268us/step - loss: 24.6532 - val_loss: 26.6552\n",
            "Epoch 164/200\n",
            "271/271 [==============================] - 0s 210us/step - loss: 25.3116 - val_loss: 25.4132\n",
            "Epoch 165/200\n",
            "271/271 [==============================] - 0s 200us/step - loss: 24.5679 - val_loss: 25.4079\n",
            "Epoch 166/200\n",
            "271/271 [==============================] - 0s 205us/step - loss: 24.2678 - val_loss: 25.9905\n",
            "Epoch 167/200\n",
            "271/271 [==============================] - 0s 194us/step - loss: 23.9219 - val_loss: 25.8337\n",
            "Epoch 168/200\n",
            "271/271 [==============================] - 0s 206us/step - loss: 24.1180 - val_loss: 25.8884\n",
            "Epoch 169/200\n",
            "271/271 [==============================] - 0s 204us/step - loss: 24.4825 - val_loss: 25.6746\n",
            "Epoch 170/200\n",
            "271/271 [==============================] - 0s 199us/step - loss: 24.6683 - val_loss: 25.6348\n",
            "Epoch 171/200\n",
            "271/271 [==============================] - 0s 199us/step - loss: 23.6043 - val_loss: 26.1166\n",
            "Epoch 172/200\n",
            "271/271 [==============================] - 0s 199us/step - loss: 24.3985 - val_loss: 25.3881\n",
            "Epoch 173/200\n",
            "271/271 [==============================] - 0s 200us/step - loss: 23.8464 - val_loss: 26.2125\n",
            "Epoch 174/200\n",
            "271/271 [==============================] - 0s 201us/step - loss: 24.8574 - val_loss: 25.9580\n",
            "Epoch 175/200\n",
            "271/271 [==============================] - 0s 201us/step - loss: 23.8949 - val_loss: 25.4719\n",
            "Epoch 176/200\n",
            "271/271 [==============================] - 0s 199us/step - loss: 24.3279 - val_loss: 25.8836\n",
            "Epoch 177/200\n",
            "271/271 [==============================] - 0s 191us/step - loss: 24.1846 - val_loss: 25.8517\n",
            "Epoch 178/200\n",
            "271/271 [==============================] - 0s 232us/step - loss: 23.6287 - val_loss: 25.4981\n",
            "Epoch 179/200\n",
            "271/271 [==============================] - 0s 201us/step - loss: 23.7951 - val_loss: 25.1250\n",
            "Epoch 180/200\n",
            "271/271 [==============================] - 0s 194us/step - loss: 24.3169 - val_loss: 25.1486\n",
            "Epoch 181/200\n",
            "271/271 [==============================] - 0s 201us/step - loss: 23.3278 - val_loss: 24.8582\n",
            "Epoch 182/200\n",
            "271/271 [==============================] - 0s 199us/step - loss: 23.5875 - val_loss: 24.4035\n",
            "Epoch 183/200\n",
            "271/271 [==============================] - 0s 204us/step - loss: 23.8753 - val_loss: 24.7151\n",
            "Epoch 184/200\n",
            "271/271 [==============================] - 0s 194us/step - loss: 23.9862 - val_loss: 26.1029\n",
            "Epoch 185/200\n",
            "271/271 [==============================] - 0s 203us/step - loss: 23.4051 - val_loss: 24.8615\n",
            "Epoch 186/200\n",
            "271/271 [==============================] - 0s 198us/step - loss: 23.4544 - val_loss: 24.3667\n",
            "Epoch 187/200\n",
            "271/271 [==============================] - 0s 200us/step - loss: 23.8085 - val_loss: 25.1441\n",
            "Epoch 188/200\n",
            "271/271 [==============================] - 0s 200us/step - loss: 23.3643 - val_loss: 24.8766\n",
            "Epoch 189/200\n",
            "271/271 [==============================] - 0s 195us/step - loss: 23.6438 - val_loss: 24.3204\n",
            "Epoch 190/200\n",
            "271/271 [==============================] - 0s 203us/step - loss: 23.1044 - val_loss: 24.3392\n",
            "Epoch 191/200\n",
            "271/271 [==============================] - 0s 204us/step - loss: 23.9891 - val_loss: 24.2906\n",
            "Epoch 192/200\n",
            "271/271 [==============================] - 0s 193us/step - loss: 23.7435 - val_loss: 24.2305\n",
            "Epoch 193/200\n",
            "271/271 [==============================] - 0s 196us/step - loss: 23.3990 - val_loss: 24.5791\n",
            "Epoch 194/200\n",
            "271/271 [==============================] - 0s 200us/step - loss: 23.0732 - val_loss: 25.9781\n",
            "Epoch 195/200\n",
            "271/271 [==============================] - 0s 208us/step - loss: 24.9477 - val_loss: 23.7846\n",
            "Epoch 196/200\n",
            "271/271 [==============================] - 0s 236us/step - loss: 23.2310 - val_loss: 24.1060\n",
            "Epoch 197/200\n",
            "271/271 [==============================] - 0s 202us/step - loss: 23.7676 - val_loss: 24.0894\n",
            "Epoch 198/200\n",
            "271/271 [==============================] - 0s 196us/step - loss: 23.2331 - val_loss: 23.8540\n",
            "Epoch 199/200\n",
            "271/271 [==============================] - 0s 201us/step - loss: 22.9449 - val_loss: 24.8656\n",
            "Epoch 200/200\n",
            "271/271 [==============================] - 0s 202us/step - loss: 23.5459 - val_loss: 24.3466\n",
            "Total Time: 17.138486 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGn8u3fTpXBv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a77f3dad-d57c-4d48-b07b-b8d23a6312a8"
      },
      "source": [
        "test_lossDense = model1.evaluate(testX, testY)\n",
        "\n",
        "preds = model1.predict(testX)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "91/91 [==============================] - 0s 118us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xv1P8OaxpW--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "diff = preds.flatten() - testY\n",
        "percentDiff = (diff / testY) * 100\n",
        "absPercentDiff = np.abs(percentDiff)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzbucyl0r-Ri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = np.mean(absPercentDiff)\n",
        "std = np.std(absPercentDiff)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xslBtSr-Or",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "45fca081-1b56-4eeb-8474-387e1dc8edb9"
      },
      "source": [
        "locale.setlocale(locale.LC_ALL, \"en_US.UTF-8\")\n",
        "print(\"[INFO] avg. house price: {}, std house price: {}\".format(locale.currency(df[\"price\"].mean(), grouping=True),locale.currency(df[\"price\"].std(), grouping=True)))\n",
        "print(\"[INFO] mean: {:.2f}%, std: {:.2f}%\".format(mean, std))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] avg. house price: $533,388.27, std house price: $493,403.08\n",
            "[INFO] mean: 21.05%, std: 22.13%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFR3n0mor-LZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "1c690943-4200-48fc-eba7-c1b93eacb374"
      },
      "source": [
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8CxfxrgSKOR",
        "colab_type": "text"
      },
      "source": [
        "## CNN with Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7PxXNwyvMDS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputPath = '/content/drive/My Drive/casas/casasBoston/Houses-dataset/Houses Dataset/'\n",
        "images = []\n",
        "\n",
        "for i in df.index.values:\n",
        "  basePath = os.path.sep.join([inputPath, \"{}_*\".format(i + 1)])\n",
        "  housePaths = sorted(list(glob.glob(basePath)))\n",
        "  inputImages = []\n",
        "  outputImage = np.zeros((64, 64, 3), dtype=\"uint8\")\n",
        "  \n",
        "  for housePath in housePaths:\n",
        "    image = cv2.imread(housePath)\n",
        "    image = cv2.resize(image, (32, 32))\n",
        "    inputImages.append(image)\n",
        "\n",
        "  outputImage[0:32, 0:32] = inputImages[0]\n",
        "  outputImage[0:32, 32:64] = inputImages[1]\n",
        "  outputImage[32:64, 32:64] = inputImages[2]\n",
        "  outputImage[32:64, 0:32] = inputImages[3]\n",
        "  images.append(outputImage)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k76huQiQ1V-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images = np.array(images)\n",
        "images = images/255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFnso8XS1V8L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(trainAttrX, testAttrX, trainImagesX, testImagesX) = train_test_split(df, images, test_size=0.25, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIb6XHxr1V5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxPrice = trainAttrX[\"price\"].max()\n",
        "trainY = trainAttrX[\"price\"] / maxPrice\n",
        "testY = testAttrX[\"price\"] / maxPrice"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32DGPLeN1V2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_cnn(width, height, depth, filters=(16, 32, 64), regress=False):\n",
        "\n",
        "\tinputShape = (height, width, depth)\n",
        "\tchanDim = -1\n",
        "\n",
        "\tinputs = Input(shape=inputShape)\n",
        "\n",
        "\tfor (i, f) in enumerate(filters):\n",
        "\t\tif i == 0:\n",
        "\t\t\tx = inputs\n",
        "\n",
        "\t\tx = Conv2D(f, (3, 3), padding=\"same\")(x)\n",
        "\t\tx = Activation(\"relu\")(x)\n",
        "\t\tx = BatchNormalization(axis=chanDim)(x)\n",
        "\t\tx = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "\tx = Flatten()(x)\n",
        "\tx = Dense(16)(x)\n",
        "\tx = Activation(\"relu\")(x)\n",
        "\tx = BatchNormalization(axis=chanDim)(x)\n",
        "\tx = Dropout(0.5)(x)\n",
        "\n",
        "\tx = Dense(4)(x)\n",
        "\tx = Activation(\"relu\")(x)\n",
        "\n",
        "\tif regress:\n",
        "\t\tx = Dense(1, activation=\"linear\")(x)\n",
        "\n",
        "\tmodel = Model(inputs, x)\n",
        "\n",
        "\treturn model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pM7nkv0W1Vzk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2 = create_cnn(64, 64, 3, regress=True)\n",
        "opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
        "model2.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dk0f3Bu21Vwr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6834
        },
        "outputId": "1fdde3da-5d61-4c9a-d986-3a5bd85da2fe"
      },
      "source": [
        "history2 = model2.fit(trainImagesX, trainY, validation_data=(testImagesX, testY),epochs=200, batch_size=8)"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 271 samples, validate on 91 samples\n",
            "Epoch 1/200\n",
            "271/271 [==============================] - 4s 14ms/step - loss: 1551.8885 - val_loss: 814.8942\n",
            "Epoch 2/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 887.2231 - val_loss: 672.2385\n",
            "Epoch 3/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 624.3974 - val_loss: 671.6264\n",
            "Epoch 4/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 569.9937 - val_loss: 536.8730\n",
            "Epoch 5/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 436.3897 - val_loss: 376.4310\n",
            "Epoch 6/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 475.2227 - val_loss: 231.9366\n",
            "Epoch 7/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 356.7876 - val_loss: 228.1142\n",
            "Epoch 8/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 304.4992 - val_loss: 106.0673\n",
            "Epoch 9/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 270.6245 - val_loss: 215.8803\n",
            "Epoch 10/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 229.5427 - val_loss: 118.6215\n",
            "Epoch 11/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 178.6971 - val_loss: 149.5596\n",
            "Epoch 12/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 179.2396 - val_loss: 80.0830\n",
            "Epoch 13/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 145.4936 - val_loss: 95.4027\n",
            "Epoch 14/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 147.8994 - val_loss: 101.6911\n",
            "Epoch 15/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 147.9108 - val_loss: 140.5786\n",
            "Epoch 16/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 127.6068 - val_loss: 84.8504\n",
            "Epoch 17/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 94.7265 - val_loss: 90.5114\n",
            "Epoch 18/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 110.7046 - val_loss: 124.9360\n",
            "Epoch 19/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 106.6903 - val_loss: 85.8270\n",
            "Epoch 20/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 122.5249 - val_loss: 89.5621\n",
            "Epoch 21/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 86.6191 - val_loss: 80.0706\n",
            "Epoch 22/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 80.6995 - val_loss: 77.0961\n",
            "Epoch 23/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 77.7078 - val_loss: 76.4179\n",
            "Epoch 24/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 88.8007 - val_loss: 77.3608\n",
            "Epoch 25/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 89.2934 - val_loss: 74.4530\n",
            "Epoch 26/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 69.2370 - val_loss: 103.4071\n",
            "Epoch 27/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 81.8281 - val_loss: 82.0026\n",
            "Epoch 28/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 68.8162 - val_loss: 74.2701\n",
            "Epoch 29/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 63.2629 - val_loss: 66.4042\n",
            "Epoch 30/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 67.7480 - val_loss: 87.0819\n",
            "Epoch 31/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 66.9784 - val_loss: 83.5129\n",
            "Epoch 32/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 70.7149 - val_loss: 76.0461\n",
            "Epoch 33/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 63.1360 - val_loss: 72.9340\n",
            "Epoch 34/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 66.5195 - val_loss: 70.2818\n",
            "Epoch 35/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 64.0901 - val_loss: 73.7587\n",
            "Epoch 36/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 58.8841 - val_loss: 67.9488\n",
            "Epoch 37/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 69.7744 - val_loss: 86.7478\n",
            "Epoch 38/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 62.4700 - val_loss: 66.4704\n",
            "Epoch 39/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 58.8695 - val_loss: 55.6882\n",
            "Epoch 40/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 70.7407 - val_loss: 63.8680\n",
            "Epoch 41/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 68.7670 - val_loss: 74.1051\n",
            "Epoch 42/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 64.4363 - val_loss: 81.2498\n",
            "Epoch 43/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 70.5937 - val_loss: 92.5122\n",
            "Epoch 44/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 59.2701 - val_loss: 80.3151\n",
            "Epoch 45/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 63.3055 - val_loss: 63.3878\n",
            "Epoch 46/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 58.1386 - val_loss: 63.6884\n",
            "Epoch 47/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 54.3337 - val_loss: 66.4243\n",
            "Epoch 48/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 55.9449 - val_loss: 63.5817\n",
            "Epoch 49/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 61.8091 - val_loss: 57.4995\n",
            "Epoch 50/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 55.7668 - val_loss: 58.8681\n",
            "Epoch 51/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 54.4583 - val_loss: 60.7933\n",
            "Epoch 52/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 60.2299 - val_loss: 62.2543\n",
            "Epoch 53/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 61.1567 - val_loss: 57.1688\n",
            "Epoch 54/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 56.9247 - val_loss: 57.4865\n",
            "Epoch 55/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 55.0208 - val_loss: 60.3659\n",
            "Epoch 56/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 59.2240 - val_loss: 64.0186\n",
            "Epoch 57/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 136.6703 - val_loss: 572.3138\n",
            "Epoch 58/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 71.1111 - val_loss: 85.1039\n",
            "Epoch 59/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 66.1274 - val_loss: 66.4789\n",
            "Epoch 60/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 63.4199 - val_loss: 63.0650\n",
            "Epoch 61/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 58.5775 - val_loss: 64.0623\n",
            "Epoch 62/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 67.1067 - val_loss: 74.7308\n",
            "Epoch 63/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 58.1827 - val_loss: 73.3136\n",
            "Epoch 64/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 60.9328 - val_loss: 76.3888\n",
            "Epoch 65/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 74.9055 - val_loss: 64.9594\n",
            "Epoch 66/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 56.9305 - val_loss: 67.0177\n",
            "Epoch 67/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 57.5635 - val_loss: 66.0646\n",
            "Epoch 68/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 50.7756 - val_loss: 61.8004\n",
            "Epoch 69/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 63.2459 - val_loss: 52.3198\n",
            "Epoch 70/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 49.1985 - val_loss: 62.2890\n",
            "Epoch 71/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 60.1246 - val_loss: 72.0527\n",
            "Epoch 72/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 70.2778 - val_loss: 70.7169\n",
            "Epoch 73/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 60.6710 - val_loss: 136.1350\n",
            "Epoch 74/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 58.0930 - val_loss: 104.3078\n",
            "Epoch 75/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 56.5900 - val_loss: 86.9507\n",
            "Epoch 76/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 55.0951 - val_loss: 68.2369\n",
            "Epoch 77/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 56.2630 - val_loss: 67.4944\n",
            "Epoch 78/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 62.9256 - val_loss: 70.4327\n",
            "Epoch 79/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 64.4765 - val_loss: 116.0356\n",
            "Epoch 80/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 59.7709 - val_loss: 75.6546\n",
            "Epoch 81/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 61.0844 - val_loss: 83.0128\n",
            "Epoch 82/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 63.5451 - val_loss: 96.1540\n",
            "Epoch 83/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 64.5268 - val_loss: 83.3415\n",
            "Epoch 84/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 55.1741 - val_loss: 71.2170\n",
            "Epoch 85/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 65.0234 - val_loss: 106.9327\n",
            "Epoch 86/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 56.6825 - val_loss: 63.4075\n",
            "Epoch 87/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 53.0186 - val_loss: 65.5283\n",
            "Epoch 88/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 52.5110 - val_loss: 60.8411\n",
            "Epoch 89/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 51.5363 - val_loss: 67.8338\n",
            "Epoch 90/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 50.4516 - val_loss: 60.3124\n",
            "Epoch 91/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 57.3175 - val_loss: 61.9759\n",
            "Epoch 92/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 49.2802 - val_loss: 66.7254\n",
            "Epoch 93/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 54.3122 - val_loss: 62.6627\n",
            "Epoch 94/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 54.2200 - val_loss: 279.4913\n",
            "Epoch 95/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 58.1285 - val_loss: 73.4381\n",
            "Epoch 96/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 49.4537 - val_loss: 64.1019\n",
            "Epoch 97/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 54.3472 - val_loss: 65.4766\n",
            "Epoch 98/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 51.4595 - val_loss: 68.0180\n",
            "Epoch 99/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 50.9889 - val_loss: 58.3182\n",
            "Epoch 100/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 47.5470 - val_loss: 67.3484\n",
            "Epoch 101/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 49.9223 - val_loss: 68.3944\n",
            "Epoch 102/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 46.8732 - val_loss: 67.9218\n",
            "Epoch 103/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 47.8370 - val_loss: 65.3952\n",
            "Epoch 104/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 56.0922 - val_loss: 105.7892\n",
            "Epoch 105/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 47.1883 - val_loss: 93.8513\n",
            "Epoch 106/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 49.2358 - val_loss: 77.6581\n",
            "Epoch 107/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 50.7360 - val_loss: 62.1904\n",
            "Epoch 108/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 54.2739 - val_loss: 55.5297\n",
            "Epoch 109/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 53.1267 - val_loss: 68.9390\n",
            "Epoch 110/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 57.0583 - val_loss: 67.8562\n",
            "Epoch 111/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 45.5436 - val_loss: 65.4454\n",
            "Epoch 112/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 51.6546 - val_loss: 56.0363\n",
            "Epoch 113/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 46.1883 - val_loss: 64.2812\n",
            "Epoch 114/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 48.0292 - val_loss: 74.2958\n",
            "Epoch 115/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 52.6410 - val_loss: 118.5459\n",
            "Epoch 116/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 57.4970 - val_loss: 89.0339\n",
            "Epoch 117/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 49.5187 - val_loss: 64.6605\n",
            "Epoch 118/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 53.8196 - val_loss: 56.8666\n",
            "Epoch 119/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 48.5312 - val_loss: 78.8305\n",
            "Epoch 120/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 57.8325 - val_loss: 60.0951\n",
            "Epoch 121/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 53.1271 - val_loss: 65.4326\n",
            "Epoch 122/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 54.1368 - val_loss: 66.2677\n",
            "Epoch 123/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 55.7714 - val_loss: 71.6035\n",
            "Epoch 124/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 80.3520 - val_loss: 105.6573\n",
            "Epoch 125/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 59.1427 - val_loss: 116.5049\n",
            "Epoch 126/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 58.7374 - val_loss: 107.1727\n",
            "Epoch 127/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 56.3464 - val_loss: 71.3084\n",
            "Epoch 128/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 57.3311 - val_loss: 67.5285\n",
            "Epoch 129/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 54.1162 - val_loss: 65.6568\n",
            "Epoch 130/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 54.3261 - val_loss: 66.4611\n",
            "Epoch 131/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 57.6121 - val_loss: 71.8993\n",
            "Epoch 132/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 55.5544 - val_loss: 64.3723\n",
            "Epoch 133/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 49.1434 - val_loss: 61.6306\n",
            "Epoch 134/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 52.6962 - val_loss: 58.0365\n",
            "Epoch 135/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 51.5775 - val_loss: 59.1979\n",
            "Epoch 136/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 49.0653 - val_loss: 59.4496\n",
            "Epoch 137/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 46.8830 - val_loss: 64.1502\n",
            "Epoch 138/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 55.4055 - val_loss: 71.9485\n",
            "Epoch 139/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 58.3677 - val_loss: 71.8773\n",
            "Epoch 140/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 56.4091 - val_loss: 71.1917\n",
            "Epoch 141/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 54.4571 - val_loss: 96.0726\n",
            "Epoch 142/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 57.7100 - val_loss: 63.2138\n",
            "Epoch 143/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 55.0332 - val_loss: 54.9611\n",
            "Epoch 144/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 51.9253 - val_loss: 70.3265\n",
            "Epoch 145/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 48.9939 - val_loss: 66.7974\n",
            "Epoch 146/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 53.9476 - val_loss: 72.0001\n",
            "Epoch 147/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 50.2110 - val_loss: 71.8293\n",
            "Epoch 148/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 49.7105 - val_loss: 69.2936\n",
            "Epoch 149/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 51.1413 - val_loss: 70.2555\n",
            "Epoch 150/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 49.2753 - val_loss: 56.6967\n",
            "Epoch 151/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 48.8556 - val_loss: 59.1031\n",
            "Epoch 152/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 55.6598 - val_loss: 60.5337\n",
            "Epoch 153/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 52.6844 - val_loss: 59.5448\n",
            "Epoch 154/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 50.8873 - val_loss: 60.0802\n",
            "Epoch 155/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 51.5762 - val_loss: 52.1658\n",
            "Epoch 156/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 53.6898 - val_loss: 58.7120\n",
            "Epoch 157/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 53.9687 - val_loss: 53.7619\n",
            "Epoch 158/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 51.7247 - val_loss: 57.3413\n",
            "Epoch 159/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 53.9495 - val_loss: 60.5514\n",
            "Epoch 160/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 53.2116 - val_loss: 53.2169\n",
            "Epoch 161/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 53.6733 - val_loss: 93.3604\n",
            "Epoch 162/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 55.6580 - val_loss: 60.7052\n",
            "Epoch 163/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 49.3507 - val_loss: 53.9938\n",
            "Epoch 164/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 54.0312 - val_loss: 56.0913\n",
            "Epoch 165/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 47.9896 - val_loss: 59.9960\n",
            "Epoch 166/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 48.1000 - val_loss: 60.5307\n",
            "Epoch 167/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 48.5227 - val_loss: 61.0632\n",
            "Epoch 168/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 47.1635 - val_loss: 58.5379\n",
            "Epoch 169/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 51.1891 - val_loss: 62.3587\n",
            "Epoch 170/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 51.0092 - val_loss: 64.3964\n",
            "Epoch 171/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 49.8803 - val_loss: 60.5056\n",
            "Epoch 172/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 53.1953 - val_loss: 59.0470\n",
            "Epoch 173/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 50.3940 - val_loss: 63.8791\n",
            "Epoch 174/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 49.4885 - val_loss: 55.8648\n",
            "Epoch 175/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 45.2957 - val_loss: 61.0353\n",
            "Epoch 176/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 49.3107 - val_loss: 111.8173\n",
            "Epoch 177/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 46.1659 - val_loss: 60.8867\n",
            "Epoch 178/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 46.6698 - val_loss: 315.4784\n",
            "Epoch 179/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 47.7223 - val_loss: 159.8206\n",
            "Epoch 180/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 48.7325 - val_loss: 234.0686\n",
            "Epoch 181/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 46.8702 - val_loss: 78.3267\n",
            "Epoch 182/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 46.0630 - val_loss: 55.0658\n",
            "Epoch 183/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 45.5678 - val_loss: 66.2883\n",
            "Epoch 184/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 55.7218 - val_loss: 64.0904\n",
            "Epoch 185/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 61.5963 - val_loss: 55.5541\n",
            "Epoch 186/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 59.1273 - val_loss: 75.8677\n",
            "Epoch 187/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 53.5531 - val_loss: 66.0785\n",
            "Epoch 188/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 50.4089 - val_loss: 55.7554\n",
            "Epoch 189/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 52.3348 - val_loss: 91.2582\n",
            "Epoch 190/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 55.3681 - val_loss: 70.8046\n",
            "Epoch 191/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 53.0751 - val_loss: 55.3652\n",
            "Epoch 192/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 52.6282 - val_loss: 61.7331\n",
            "Epoch 193/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 46.2782 - val_loss: 69.1204\n",
            "Epoch 194/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 47.8552 - val_loss: 62.9820\n",
            "Epoch 195/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 56.2509 - val_loss: 53.7207\n",
            "Epoch 196/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 54.0154 - val_loss: 51.7734\n",
            "Epoch 197/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 47.6523 - val_loss: 58.5388\n",
            "Epoch 198/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 48.6511 - val_loss: 62.7071\n",
            "Epoch 199/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 54.0297 - val_loss: 57.0843\n",
            "Epoch 200/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 47.1884 - val_loss: 52.9386\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f4zZl-X1Vig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = model2.predict(testImagesX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwshZkAc558f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "diff = preds.flatten() - testY\n",
        "percentDiff = (diff / testY) * 100\n",
        "absPercentDiff = np.abs(percentDiff)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdKRcIf7555t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = np.mean(absPercentDiff)\n",
        "std = np.std(absPercentDiff)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uu8-EOlN5516",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4ff67930-01a4-4f84-a01d-9aa013aa234a"
      },
      "source": [
        "locale.setlocale(locale.LC_ALL, \"en_US.UTF-8\")\n",
        "print(\"[INFO] avg. house price: {}, std house price: {}\".format(\n",
        "    locale.currency(df[\"price\"].mean(), grouping=True),\n",
        "    locale.currency(df[\"price\"].std(), grouping=True)))\n",
        "print(\"[INFO] mean: {:.2f}%, std: {:.2f}%\".format(mean, std))"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] avg. house price: $533,388.27, std house price: $493,403.08\n",
            "[INFO] mean: 52.94%, std: 91.42%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5l_qLqRqSRKi",
        "colab_type": "text"
      },
      "source": [
        "## Combine the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl1pQ0Z7-cpt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "outputId": "84a0504e-c2cd-436e-8b3c-5a294a98a6d2"
      },
      "source": [
        "continuous = [\"bedrooms\", \"bathrooms\", \"area\"]\n",
        "cs = MinMaxScaler()\n",
        "trainContinuous = cs.fit_transform(train[continuous])\n",
        "testContinuous = cs.transform(test[continuous])\n",
        "\n",
        "zipBinarizer = LabelBinarizer().fit(df[\"zipcode\"])\n",
        "trainCategorical = zipBinarizer.transform(train[\"zipcode\"])\n",
        "testCategorical = zipBinarizer.transform(test[\"zipcode\"])\n",
        "\n",
        "trainX = np.hstack([trainCategorical, trainContinuous])\n",
        "testX = np.hstack([testCategorical, testContinuous])"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
            "  return self.partial_fit(X, y)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yic8coQZAZgG",
        "colab": {}
      },
      "source": [
        "inputPath = '/content/drive/My Drive/casas/casasBoston/Houses-dataset/Houses Dataset/'\n",
        "images = []\n",
        "\n",
        "for i in df.index.values:\n",
        "  basePath = os.path.sep.join([inputPath, \"{}_*\".format(i + 1)])\n",
        "  housePaths = sorted(list(glob.glob(basePath)))\n",
        "  inputImages = []\n",
        "  outputImage = np.zeros((64, 64, 3), dtype=\"uint8\")\n",
        "  \n",
        "  for housePath in housePaths:\n",
        "    image = cv2.imread(housePath)\n",
        "    image = cv2.resize(image, (32, 32))\n",
        "    inputImages.append(image)\n",
        "\n",
        "  outputImage[0:32, 0:32] = inputImages[0]\n",
        "  outputImage[0:32, 32:64] = inputImages[1]\n",
        "  outputImage[32:64, 32:64] = inputImages[2]\n",
        "  outputImage[32:64, 0:32] = inputImages[3]\n",
        "  \n",
        "  images.append(outputImage)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZXAYyk8DAZgK",
        "colab": {}
      },
      "source": [
        "images = np.array(images)\n",
        "images = images/255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B_9GuHLcAZgN",
        "colab": {}
      },
      "source": [
        "(trainAttrX, testAttrX, trainImagesX, testImagesX) = train_test_split(df, images, test_size=0.25, random_state=42)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gHFlrsgDAZgP",
        "colab": {}
      },
      "source": [
        "maxPrice = trainAttrX[\"price\"].max()\n",
        "trainY = trainAttrX[\"price\"] / maxPrice\n",
        "testY = testAttrX[\"price\"] / maxPrice"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pztuBP0XCqNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_mlp(dim, regress=False):\n",
        "\t# define our MLP network\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(8, input_dim=dim, activation=\"relu\"))\n",
        "\tmodel.add(Dense(4, activation=\"relu\"))\n",
        "  \n",
        "  if regress:\n",
        "\t\tmodel.add(Dense(1, activation=\"linear\"))\n",
        "\n",
        "\treturn model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLFAVz2Q8U-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp = create_mlp(trainAttrX.shape[1], regress=False)\n",
        "cnn = create_cnn(64, 64, 3, regress=False)\n",
        "\n",
        "combinedInput = concatenate([mlp.output, cnn.output])\n",
        " \n",
        "x = Dense(4, activation=\"relu\")(combinedInput)\n",
        "x = Dense(1, activation=\"linear\")(x)\n",
        " \n",
        "modelF = Model(inputs=[mlp.input, cnn.input], outputs=x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vy7Wdgho8U7c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6834
        },
        "outputId": "2194d61c-fca0-4689-8d4f-b794229c98e9"
      },
      "source": [
        "opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
        "modelF.compile(loss=\"mape\", optimizer=opt)\n",
        " \n",
        "modelF.fit([trainAttrX, trainImagesX], trainY, validation_data=([testAttrX, testImagesX], testY),epochs=200, batch_size=8)\n",
        "\n",
        "preds = modelF.predict([testAttrX, testImagesX])"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 271 samples, validate on 91 samples\n",
            "Epoch 1/200\n",
            "271/271 [==============================] - 3s 12ms/step - loss: 32.3720 - val_loss: 1110.1322\n",
            "Epoch 2/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 34.6896 - val_loss: 1062.5939\n",
            "Epoch 3/200\n",
            "271/271 [==============================] - 0s 2ms/step - loss: 34.7522 - val_loss: 1014.1627\n",
            "Epoch 4/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 29.8905 - val_loss: 997.1898\n",
            "Epoch 5/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 29.0915 - val_loss: 1374.5458\n",
            "Epoch 6/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 30.7741 - val_loss: 191947.3870\n",
            "Epoch 7/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.9804 - val_loss: 209.5709\n",
            "Epoch 8/200\n",
            "271/271 [==============================] - 0s 2ms/step - loss: 33.1930 - val_loss: 169172.7793\n",
            "Epoch 9/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 26.6814 - val_loss: 516000.2905\n",
            "Epoch 10/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 35.1154 - val_loss: 157.1932\n",
            "Epoch 11/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 26.3900 - val_loss: 2043.1555\n",
            "Epoch 12/200\n",
            "271/271 [==============================] - 0s 2ms/step - loss: 28.3183 - val_loss: 252.3044\n",
            "Epoch 13/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 29.3603 - val_loss: 259.6111\n",
            "Epoch 14/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 30.8909 - val_loss: 190.4313\n",
            "Epoch 15/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.8897 - val_loss: 130852.2715\n",
            "Epoch 16/200\n",
            "271/271 [==============================] - 0s 2ms/step - loss: 32.7317 - val_loss: 3116.6125\n",
            "Epoch 17/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 32.2806 - val_loss: 5975.0605\n",
            "Epoch 18/200\n",
            "271/271 [==============================] - 0s 2ms/step - loss: 34.3399 - val_loss: 72.0125\n",
            "Epoch 19/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 26.0745 - val_loss: 73.9809\n",
            "Epoch 20/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 29.6409 - val_loss: 170.0368\n",
            "Epoch 21/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 29.0705 - val_loss: 213.3403\n",
            "Epoch 22/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 30.0341 - val_loss: 251.7113\n",
            "Epoch 23/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 29.7284 - val_loss: 339.9862\n",
            "Epoch 24/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 30.5139 - val_loss: 275.7029\n",
            "Epoch 25/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 31.4161 - val_loss: 4039.0896\n",
            "Epoch 26/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 31.4421 - val_loss: 87.3666\n",
            "Epoch 27/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.5441 - val_loss: 72.6859\n",
            "Epoch 28/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 32.3697 - val_loss: 74.2020\n",
            "Epoch 29/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 31.1550 - val_loss: 72.1168\n",
            "Epoch 30/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.0853 - val_loss: 82.5133\n",
            "Epoch 31/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 30.9894 - val_loss: 1279.0704\n",
            "Epoch 32/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 30.7689 - val_loss: 1347.6659\n",
            "Epoch 33/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 30.7816 - val_loss: 199.2858\n",
            "Epoch 34/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.9530 - val_loss: 1159.5842\n",
            "Epoch 35/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 29.1300 - val_loss: 63062.3828\n",
            "Epoch 36/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.5050 - val_loss: 1560.8692\n",
            "Epoch 37/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 29.7819 - val_loss: 1237.9059\n",
            "Epoch 38/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.3192 - val_loss: 1341.7475\n",
            "Epoch 39/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 26.2423 - val_loss: 543313.2637\n",
            "Epoch 40/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 29.3651 - val_loss: 900793.6291\n",
            "Epoch 41/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 31.4928 - val_loss: 2207676.4657\n",
            "Epoch 42/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.4581 - val_loss: 2925210.2898\n",
            "Epoch 43/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 29.0787 - val_loss: 2592396.7266\n",
            "Epoch 44/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 29.1084 - val_loss: 2002845.4169\n",
            "Epoch 45/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 29.2752 - val_loss: 1398999.9368\n",
            "Epoch 46/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 33.9959 - val_loss: 150507.1197\n",
            "Epoch 47/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.1395 - val_loss: 20672.9070\n",
            "Epoch 48/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 27.0816 - val_loss: 173970.2601\n",
            "Epoch 49/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 32.6460 - val_loss: 72.7710\n",
            "Epoch 50/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 27.5760 - val_loss: 43506.0293\n",
            "Epoch 51/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 26.9335 - val_loss: 71.8259\n",
            "Epoch 52/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.3784 - val_loss: 82.9206\n",
            "Epoch 53/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 26.1495 - val_loss: 27002.2063\n",
            "Epoch 54/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.5998 - val_loss: 152692.2282\n",
            "Epoch 55/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 26.8368 - val_loss: 4431.5164\n",
            "Epoch 56/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 33.9690 - val_loss: 66.5615\n",
            "Epoch 57/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 31.0137 - val_loss: 72.4043\n",
            "Epoch 58/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 30.6405 - val_loss: 72.3519\n",
            "Epoch 59/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 31.0403 - val_loss: 260653.3111\n",
            "Epoch 60/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 27.1762 - val_loss: 500248.7543\n",
            "Epoch 61/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 30.7371 - val_loss: 213132.4962\n",
            "Epoch 62/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 29.9417 - val_loss: 494646.5014\n",
            "Epoch 63/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 32.7042 - val_loss: 213906.4066\n",
            "Epoch 64/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 32.2552 - val_loss: 2514.6625\n",
            "Epoch 65/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 30.3462 - val_loss: 9595.0994\n",
            "Epoch 66/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 29.3006 - val_loss: 9796.4133\n",
            "Epoch 67/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 31.7398 - val_loss: 4726.4189\n",
            "Epoch 68/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 31.6659 - val_loss: 1416.6773\n",
            "Epoch 69/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 30.7982 - val_loss: 205.3607\n",
            "Epoch 70/200\n",
            "271/271 [==============================] - 0s 2ms/step - loss: 28.8496 - val_loss: 19011.1808\n",
            "Epoch 71/200\n",
            "271/271 [==============================] - 0s 2ms/step - loss: 31.5642 - val_loss: 250.0412\n",
            "Epoch 72/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 29.8597 - val_loss: 87.1449\n",
            "Epoch 73/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 27.9675 - val_loss: 64.6425\n",
            "Epoch 74/200\n",
            "271/271 [==============================] - 0s 2ms/step - loss: 34.9594 - val_loss: 47.8225\n",
            "Epoch 75/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 29.4385 - val_loss: 58.0946\n",
            "Epoch 76/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 29.5778 - val_loss: 62.8832\n",
            "Epoch 77/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 26.8162 - val_loss: 58.6120\n",
            "Epoch 78/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.4829 - val_loss: 4210.8172\n",
            "Epoch 79/200\n",
            "271/271 [==============================] - 0s 2ms/step - loss: 27.4330 - val_loss: 71.3042\n",
            "Epoch 80/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 27.3342 - val_loss: 87.9779\n",
            "Epoch 81/200\n",
            "271/271 [==============================] - 0s 2ms/step - loss: 28.2256 - val_loss: 53.2840\n",
            "Epoch 82/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 34.4062 - val_loss: 64.8931\n",
            "Epoch 83/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 30.5816 - val_loss: 100.5061\n",
            "Epoch 84/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.7566 - val_loss: 393.7093\n",
            "Epoch 85/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 25.1654 - val_loss: 300.3293\n",
            "Epoch 86/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 26.8037 - val_loss: 86.5800\n",
            "Epoch 87/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.7690 - val_loss: 113.1391\n",
            "Epoch 88/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 27.6202 - val_loss: 65.8442\n",
            "Epoch 89/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 31.1466 - val_loss: 178.9825\n",
            "Epoch 90/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 26.6777 - val_loss: 235.0609\n",
            "Epoch 91/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 29.5767 - val_loss: 317.9442\n",
            "Epoch 92/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 29.2174 - val_loss: 132.2366\n",
            "Epoch 93/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 32.9190 - val_loss: 191.5918\n",
            "Epoch 94/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 26.3552 - val_loss: 103.3867\n",
            "Epoch 95/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 29.1392 - val_loss: 257.4961\n",
            "Epoch 96/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 26.6368 - val_loss: 410.2241\n",
            "Epoch 97/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 32.3475 - val_loss: 319.3352\n",
            "Epoch 98/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.2184 - val_loss: 348.5752\n",
            "Epoch 99/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.9173 - val_loss: 176.6688\n",
            "Epoch 100/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 27.1926 - val_loss: 271.7313\n",
            "Epoch 101/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 31.7545 - val_loss: 259.5993\n",
            "Epoch 102/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.2351 - val_loss: 175.8601\n",
            "Epoch 103/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 29.3398 - val_loss: 164.8152\n",
            "Epoch 104/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 30.5998 - val_loss: 373.3156\n",
            "Epoch 105/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 26.2258 - val_loss: 473.5063\n",
            "Epoch 106/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.6269 - val_loss: 327.6350\n",
            "Epoch 107/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 26.6952 - val_loss: 102.2550\n",
            "Epoch 108/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.2292 - val_loss: 143.1105\n",
            "Epoch 109/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 30.9889 - val_loss: 519.4846\n",
            "Epoch 110/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 27.3952 - val_loss: 417.7435\n",
            "Epoch 111/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 27.9471 - val_loss: 134.3653\n",
            "Epoch 112/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 29.0699 - val_loss: 45798.1748\n",
            "Epoch 113/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.1275 - val_loss: 3705.6173\n",
            "Epoch 114/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 27.9558 - val_loss: 174.8948\n",
            "Epoch 115/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 26.9685 - val_loss: 156.6997\n",
            "Epoch 116/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.5776 - val_loss: 54.0852\n",
            "Epoch 117/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.3944 - val_loss: 14674.3571\n",
            "Epoch 118/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.0464 - val_loss: 72.8333\n",
            "Epoch 119/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 29.2772 - val_loss: 65.8917\n",
            "Epoch 120/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 29.1287 - val_loss: 62.9234\n",
            "Epoch 121/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.7710 - val_loss: 70.7843\n",
            "Epoch 122/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 27.1765 - val_loss: 68.3740\n",
            "Epoch 123/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 31.7580 - val_loss: 59.3005\n",
            "Epoch 124/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.8750 - val_loss: 66.6439\n",
            "Epoch 125/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.6737 - val_loss: 61.6694\n",
            "Epoch 126/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 31.4829 - val_loss: 65.5474\n",
            "Epoch 127/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.7781 - val_loss: 55.8514\n",
            "Epoch 128/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.8562 - val_loss: 54.0255\n",
            "Epoch 129/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.1982 - val_loss: 65.5267\n",
            "Epoch 130/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 32.7780 - val_loss: 164.6996\n",
            "Epoch 131/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 29.8120 - val_loss: 141.8942\n",
            "Epoch 132/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 29.1749 - val_loss: 90.2797\n",
            "Epoch 133/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 31.1888 - val_loss: 86.3721\n",
            "Epoch 134/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 26.7071 - val_loss: 72.1520\n",
            "Epoch 135/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 25.2614 - val_loss: 63.7101\n",
            "Epoch 136/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 27.5181 - val_loss: 54.5741\n",
            "Epoch 137/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 26.9106 - val_loss: 58226.9354\n",
            "Epoch 138/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 26.9230 - val_loss: 27912.0788\n",
            "Epoch 139/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.0631 - val_loss: 153.2511\n",
            "Epoch 140/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 27.9489 - val_loss: 118.4395\n",
            "Epoch 141/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 29.6959 - val_loss: 54906.1987\n",
            "Epoch 142/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.1368 - val_loss: 29651.9911\n",
            "Epoch 143/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 26.6251 - val_loss: 91.9033\n",
            "Epoch 144/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.8443 - val_loss: 197.0919\n",
            "Epoch 145/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 31.9195 - val_loss: 334.0202\n",
            "Epoch 146/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 27.0041 - val_loss: 218.1755\n",
            "Epoch 147/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 26.0565 - val_loss: 139.1942\n",
            "Epoch 148/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 29.0285 - val_loss: 20704.6351\n",
            "Epoch 149/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 33.5192 - val_loss: 204.3946\n",
            "Epoch 150/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 30.3401 - val_loss: 113.3576\n",
            "Epoch 151/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 29.6960 - val_loss: 102.8670\n",
            "Epoch 152/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 27.4227 - val_loss: 5829.1283\n",
            "Epoch 153/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 26.0103 - val_loss: 383.3755\n",
            "Epoch 154/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 25.1623 - val_loss: 66972.5794\n",
            "Epoch 155/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.7203 - val_loss: 373.3973\n",
            "Epoch 156/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 27.3179 - val_loss: 184.3870\n",
            "Epoch 157/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.5808 - val_loss: 177.2042\n",
            "Epoch 158/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 27.3751 - val_loss: 163.5653\n",
            "Epoch 159/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 27.4639 - val_loss: 340.4108\n",
            "Epoch 160/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 26.0239 - val_loss: 208.6016\n",
            "Epoch 161/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 26.7246 - val_loss: 65.3853\n",
            "Epoch 162/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 27.2011 - val_loss: 61.7931\n",
            "Epoch 163/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 27.5859 - val_loss: 110.4121\n",
            "Epoch 164/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 29.9350 - val_loss: 447.9534\n",
            "Epoch 165/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 32.1971 - val_loss: 1886.6280\n",
            "Epoch 166/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 29.5746 - val_loss: 872.4192\n",
            "Epoch 167/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 27.1315 - val_loss: 502.6757\n",
            "Epoch 168/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 27.9079 - val_loss: 529.6525\n",
            "Epoch 169/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 30.3730 - val_loss: 332.7019\n",
            "Epoch 170/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 26.8043 - val_loss: 231.3317\n",
            "Epoch 171/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.0205 - val_loss: 227.3962\n",
            "Epoch 172/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 27.2964 - val_loss: 72.5868\n",
            "Epoch 173/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 27.6072 - val_loss: 62.4596\n",
            "Epoch 174/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 31.6811 - val_loss: 62.7315\n",
            "Epoch 175/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 29.7873 - val_loss: 18318.4702\n",
            "Epoch 176/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 29.5677 - val_loss: 5779.0447\n",
            "Epoch 177/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 26.8794 - val_loss: 7082.8588\n",
            "Epoch 178/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 27.7702 - val_loss: 1291.3638\n",
            "Epoch 179/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 26.2632 - val_loss: 64.5832\n",
            "Epoch 180/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 25.9740 - val_loss: 58.9107\n",
            "Epoch 181/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 27.5542 - val_loss: 11148.7420\n",
            "Epoch 182/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.3267 - val_loss: 9189.7562\n",
            "Epoch 183/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 25.3342 - val_loss: 10201.6359\n",
            "Epoch 184/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 26.2290 - val_loss: 31940.6841\n",
            "Epoch 185/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 26.8191 - val_loss: 14988.4235\n",
            "Epoch 186/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 30.1591 - val_loss: 78807.3202\n",
            "Epoch 187/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 26.5665 - val_loss: 231268.5497\n",
            "Epoch 188/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.3009 - val_loss: 124668.3910\n",
            "Epoch 189/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.1791 - val_loss: 22033.2186\n",
            "Epoch 190/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 29.3846 - val_loss: 43934.0647\n",
            "Epoch 191/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 29.3446 - val_loss: 65.8910\n",
            "Epoch 192/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 27.9355 - val_loss: 65.7432\n",
            "Epoch 193/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 27.0791 - val_loss: 61.9499\n",
            "Epoch 194/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 28.4120 - val_loss: 61.2521\n",
            "Epoch 195/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 25.4272 - val_loss: 239.2628\n",
            "Epoch 196/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 25.6778 - val_loss: 92.5004\n",
            "Epoch 197/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 26.0455 - val_loss: 1061.4321\n",
            "Epoch 198/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 27.2467 - val_loss: 13265.9253\n",
            "Epoch 199/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 25.5553 - val_loss: 8381.9433\n",
            "Epoch 200/200\n",
            "271/271 [==============================] - 0s 1ms/step - loss: 26.1541 - val_loss: 824.6117\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGObQdO18UzW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4d47ecd3-5185-4ca4-cbe7-39727aa7dd3c"
      },
      "source": [
        "diff = preds.flatten() - testY\n",
        "percentDiff = (diff / testY) * 100\n",
        "absPercentDiff = np.abs(percentDiff)\n",
        " \n",
        "\n",
        "mean = np.mean(absPercentDiff)\n",
        "std = np.std(absPercentDiff)\n",
        " \n",
        "locale.setlocale(locale.LC_ALL, \"en_US.UTF-8\")\n",
        "print(\"[INFO] avg. house price: {}, std house price: {}\".format(\n",
        "    locale.currency(df[\"price\"].mean(), grouping=True),\n",
        "    locale.currency(df[\"price\"].std(), grouping=True)))\n",
        "print(\"[INFO] mean: {:.2f}%, std: {:.2f}%\".format(mean, std))"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] avg. house price: $533,388.27, std house price: $493,403.08\n",
            "[INFO] mean: 824.61%, std: 2635.13%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqhpzTqATdtG",
        "colab_type": "text"
      },
      "source": [
        "# Sources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBeLn6I7neA1",
        "colab_type": "text"
      },
      "source": [
        "Source: H. Ahmed E. and Moustafa M. (2016). House Price Estimation from Visual and Textual Features.In Proceedings of the 8th International Joint Conference on Computational Intelligence (IJCCI 2016)ISBN 978-989-758-201-1, pages 62-68. DOI: 10.5220/0006040700620068"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2EF18jFTk-p",
        "colab_type": "text"
      },
      "source": [
        "https://www.pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sPMkGOjTmVG",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}